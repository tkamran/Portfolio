# -*- coding: utf-8 -*-
"""Taxi-Trip-Prediction-Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13v-vrnp-lr1V4HNvOiWPEPPg9ZcIte_o

#NYC Taxi Trip Duration Prediction Dataset

# Data Overview

## Features

There are a total of 10 features that are provided by the dataset which are listed below.

**id** - unique identifier for each taxi trip

**vendor_id** - taxi trip providers

**pickup_datetime** - when the trip started

**dropoff_datetime** - when the trip ended

**passenger_count** - number of passengers for that specific trip

**pickup_longitude** - longitude when the trip started

**pickup_latitude** - latitude when the trip started

**dropoff_longitude** - longitude when the trip ended

**dropoff_latitude** - latitude when the trip ended

**store_and_fwd_flag** - This flag shows whether or not the trip record was held in vehicle memory because a connection could not be established from the vehicle to the server.


## Target

Our target is **trip_duration** which is how long a specific NYC taxi trip lasted in seconds.


### Dataset Source
https://www.kaggle.com/c/nyc-taxi-trip-duration/data
"""

# Commented out IPython magic to ensure Python compatibility.
#importing all necessary modules here
!pip install haversine
import pandas as pd
import numpy as np
import haversine.haversine as hs
import matplotlib.pyplot as plt

# %matplotlib inline

# Downloading train.csv from google drive
# https://drive.google.com/file/d/1fw3FpX7yck7zYZO9tuoFznDk6ZccsgfJ/view?usp=sharing - Google Drive Link
!gdown --id 1fw3FpX7yck7zYZO9tuoFznDk6ZccsgfJ

train_df = pd.read_csv("/content/train.csv")

# features = df.columns[:-1]
# X = df[features]
# y = df["trip_duration"]

"""# Data Analysis

First, we will check how many data samples there are in this dataset.
"""

print(f'The number of data samples provided is: {train_df.shape[0]}')

features = ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'passenger_count','pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag']

for feature in features:
  print(f'Unique values  - {feature} : {train_df[feature].unique().shape[0]}')

"""We must look for anything interesting about this dataset."""

train_df.describe()

"""We see that the minimum value for passenger count is 0 which is interesting because it likely indicates that the trip was cancelled or it was accidentally recorded. Perhaps it may be a good idea to drop the rows that contain trips that are cancelled or mistakenly recorded because they  do not contribute anything to the prediction of taxi trips.

A similar observation can also be made with the minimum value of trip duration of one second since it is likely a cancelled trip.

Additionally, the max value of trip duration is interesting because it is an extreme outlier so we should look into the longer trips to investigate.
"""

print(train_df['passenger_count'].value_counts())

train_df = train_df[train_df['passenger_count'] <= 6]
train_df = train_df[train_df['passenger_count'] != 0]


train_df.head()

#define a function that returns a list of indices of the outliers
def find_outliers(df, ft):
    Q1 = df[ft].quantile(0.25)
    Q3 = df[ft].quantile(0.75)
    IQR = Q3 - Q1
    
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    
    outliers = df.index[ (df[ft] < lower) | (df[ft] > upper)]
    
    return outliers

#define a function which returns a cleaned dataframe without outliers
def remove_outliers(dataframe, outliers):
    outliers = sorted(set(outliers))
    dataframe = dataframe.drop(outliers)
    
    return dataframe

print(train_df.nlargest(10, ['trip_duration']))


td_outliers = find_outliers(train_df, 'trip_duration')
train_df = remove_outliers(train_df, td_outliers)

train_df = train_df[train_df['trip_duration'] >= 10]
train_df['trip_duration'].describe()

"""We must figure out how to handle the given longitude and latitude columns, so our idea was to convert the given coordinates into a distance value.

To get the given distance between the coordinates, we found out there was a formula called haversine which can be used to calculate the distance.

To use the haversine formula, we installed a module called haversine which we plugged into the apply function to create our new distance feature as shown below.

Now we will use this feature in our models and drop the longitude and latitude columns.
"""

train_df['distance'] = train_df.apply(lambda row: hs((row['pickup_latitude'], 
                                               row['pickup_longitude']),
                                               (row['dropoff_latitude'],
                                               row['dropoff_longitude'])), axis = 1)


train_df = train_df.drop(['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'], axis = 1)

train_df.head()

train_df = train_df[train_df['distance'] != 0]

train_df

#Speed feature in km/h
train_df['speed'] = train_df.distance / (train_df.trip_duration / 3600)

train_df = train_df[train_df['speed'] <= 160]

"""Now, we must handle the datetime columns in our dataset.

First, we will convert the strings into their appropriate datetime objects.

Then,

We will be able to extract specific information such as the month and day of the week.

We will also convert the given time into parts of the day such as Morning, Afternoon, Evening, Night.
"""

print(train_df.dtypes) #Types before

train_df['pickup_datetime'] = pd.to_datetime(train_df['pickup_datetime'])
train_df['dropoff_datetime'] = pd.to_datetime(train_df['dropoff_datetime'])

print(train_df.dtypes) #Types after

train_df['month'] = train_df['pickup_datetime'].dt.month_name()
train_df['day_of_week'] = train_df['pickup_datetime'].dt.day_name()
train_df['pickup_hour'] = train_df['pickup_datetime'].dt.hour
train_df['dropoff_hour'] = train_df['dropoff_datetime'].dt.hour

# Morning 5 am - 12 pm
# Afternoon 12 pm - 5 pm
# Evening 5 pm - 9 pm
# Night 9 pm - 4 am


def timeperiod(hour):
  hour = hour + 1
  if hour >= 5 and hour < 12:
    return 'morning'
  elif hour >= 12 and hour < 17:
    return 'afternoon'
  elif hour >= 17 and hour < 21:
    return 'evening'
  elif hour >= 21 or hour < 5:
    return 'night'

train_df['pickup_hour'] = train_df['pickup_hour'].apply(timeperiod)
train_df['dropoff_hour'] = train_df['dropoff_hour'].apply(timeperiod)
train_df

print(train_df.dtypes)

#categorical columns to onehotencode

categorical = ['month', 'day_of_week', 'pickup_hour', 'dropoff_hour']

train_df = pd.get_dummies(train_df, columns=categorical)

train_df = train_df.drop(columns=['id','pickup_datetime','dropoff_datetime', 'vendor_id', 'store_and_fwd_flag'])

train_df

y = train_df['trip_duration'];
X = train_df.drop(columns=['trip_duration']);

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=35)

"""# Linear Regression"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

my_linreg = LinearRegression()
mse_list = cross_val_score(my_linreg, X, y, cv=10, scoring='neg_mean_squared_error')
mse_list_positive = -mse_list
rmse_list = np.sqrt(mse_list_positive)

rmse_linreg = rmse_list.mean()
print(rmse_linreg)

my_linreg = None

"""# XGBoost Regressor"""

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error as MSE

my_xgboost = XGBRegressor(n_estimators = 200, random_state=35, objective='reg:squarederror')
my_xgboost.fit(X_train, y_train)
rmse_xgboost = np.sqrt(MSE(y_test, my_xgboost.predict(X_test)))
print(rmse_xgboost)

my_xgboost = None

"""# AdaBoost Regressor"""

from sklearn.ensemble import AdaBoostRegressor

my_adaboost = AdaBoostRegressor(n_estimators = 100, random_state = 35)
my_adaboost.fit(X_train, y_train)
rmse_adaboost = np.sqrt(MSE(y_test, my_adaboost.predict(X_test)))
print(rmse_adaboost)

my_adaboost = None

"""# Random Forest Regressor

"""

from sklearn.ensemble import RandomForestRegressor

my_rf = RandomForestRegressor(random_state = 35)
my_rf.fit(X_train, y_train)
rmse_rf = np.sqrt(MSE(y_test, my_rf.predict(X_test)))
print(rmse_rf)

"""# Neural Network Model

## PCA
"""

from sklearn import preprocessing
from sklearn.decomposition import PCA

X = preprocessing.scale(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=35)

pca = PCA(n_components = 4)
X_train_pca = pca.fit_transform(X_train) 
X_test_pca = pca.transform(X_test)

"""## Neural Network Model"""

from sklearn.neural_network import MLPRegressor

my_ANN = MLPRegressor(hidden_layer_sizes=(30,30), activation= 'relu', 
                       solver='adam', alpha=1, random_state=35, 
                       learning_rate_init = 0.008) 
my_ANN.fit(X_train_pca, y_train)
rmse_ann = np.sqrt(MSE(y_test, my_ANN.predict(X_test_pca))) 
print(rmse_ann)

"""# Summary of Results"""

results = [rmse_linreg, rmse_xgboost, rmse_adaboost, rmse_rf, rmse_ann]
results = [round(x,3) for x in results]

labels = ["Linear Regression", "XGBoost", "AdaBoost", "Random Forest", "ANN"]

bars = plt.bar(labels, results, color ='red', width = 0.5) 
plt.xlabel("Regressors") 
plt.ylabel("RMSE Results") 
plt.title("RMSE of Each Regressor Used") 


for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + 0.05, height + 4, height)

plt.show()